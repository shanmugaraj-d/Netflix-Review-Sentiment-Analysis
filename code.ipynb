{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xIZKVAnnA-U",
        "outputId": "4b0e71fd-2109-4050-ec1c-71ce6e215227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m\u2714 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m\u26a0 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Optimized Netflix Review Sentiment Classification\n",
        "\n",
        "# Step 1: Install & Import Libraries\n",
        "!pip install -q transformers datasets nltk spacy scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import TFRobertaForSequenceClassification, RobertaTokenizer, create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Download NLP resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Load Dataset\n",
        "data = pd.read_csv('netflix_reviews.csv')\n",
        "data['content'] = data['content'].fillna('').astype(str)\n"
      ],
      "metadata": {
        "id": "RzDJbZVZnDhU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Clean Text\n",
        "def clean_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    return ' '.join([token for token in tokens if token not in stop_words])\n",
        "\n",
        "data['processed_content'] = data['content'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "uDzEXyU1nXTl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Label Encoding\n",
        "data['label'] = data['score'].apply(lambda x: 1 if x >= 3 else 0)\n"
      ],
      "metadata": {
        "id": "TLhv6fcqnpWp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train-Test Split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['processed_content'], data['label'], test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "u-0KkpHSprrX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Tokenization for RoBERTa\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def encode(texts):\n",
        "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "train_enc = encode(train_texts)\n",
        "test_enc = encode(test_texts)\n",
        "\n",
        "train_labels_tf = tf.convert_to_tensor(train_labels.values)\n",
        "test_labels_tf = tf.convert_to_tensor(test_labels.values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "529c43af0a7141278eb63b72c7e7cb89",
            "77e764c8e2274abfbee3241a3cc5c96e",
            "e1f4864908f14905ba74edad006cccb3",
            "685a7ef50c8b4b39913cae05a34e8b7a",
            "cb944d4d8dca4f90b8b9e0ccbb1f3f19",
            "031b5cc7160e43f79ec83a83a0fd307b",
            "bdeddda3a86644fe9bf64d42166a16cb",
            "6e898f10673d409aab1ff0cc37c8a096",
            "79a650a1e7484b539551d8d7253b1d31",
            "d40ff0be79df4ddb8fed38a30a577dbc",
            "81bc7a52167c42d58efc4eb93118edab",
            "716d9e69e156456f8bfddce347fdcb6b",
            "fb2c618fe1274d3c8872e152adbf3047",
            "30f5c056c52a468fb61e64d2c7ea63a3",
            "75a3e26add6c4631b0ead06967225066",
            "4ebd0c0a927247328e996bbcda6789fd",
            "f0a773a3508a43ff9fe67bf1a3d2e8a4",
            "411f8bb5e12445c09ba87bc2ed0477b6",
            "d0668c8843734ebe92307759dcf30a12",
            "754b327ef0e246da9e03fbea45161166",
            "bf46b5b3646249088c750f295ba8aace",
            "6e02b1a119e44589a537df588929f8ee",
            "17f55471fabc47ac91033a719fd305d4",
            "6868775f73c24c19a97eeb30d4f878dc",
            "1c47fed240594aefb18e277e894ec956",
            "f44656bf9ca349f6b7f594264306a214",
            "68971cea8f30464f89b08ddc6c540e08",
            "83ed3e6abb1842cf81be3952b4a9afb9",
            "89b8170b020c404883daf2eb9a558b0e",
            "8b9c6016d7004ae188181eadb6500981",
            "32fb58ad104e4be499bfa431089f3071",
            "31f3cebc2a4e4ff085555b36cf98a160",
            "cb1079b87b4e4cbab7a13818a2853f9a",
            "11c0185b048f45909d2686f3b3c8b1fc",
            "1535f1da1a3f408e80f4c22d4d6028e9",
            "bb06f28c6f5a4ff5a8d2e9c23c655d73",
            "e9a7e0944b5f4a8e8bb057f34c2d0ff6",
            "430b54f985aa483b80d8227e2594c193",
            "3ef8ce64da4b4957ac292685c1c397af",
            "81924cfe693644319695a92409d27637",
            "f158e028d3c2410ba46bbeacbeee1b53",
            "a89bfa09b5fb40ecb559fd40106ed13e",
            "664715f7ab1740a187b2a46ae99b9c5d",
            "1e21e7d7f131466fa568b3329b114a00",
            "0990effb404648b883fb0ad3f272f251",
            "5900610bc89145f08d767173a278fb1e",
            "7cb880ba2f194fa99c80811a4ded1985",
            "415e11f9c01c4349b90161a3b37f0e26",
            "95b12d9f8cbb44cb90f22eb826ac6b4b",
            "2e01996c0df042f880702a4eeb66ba3b",
            "655a3251d8bc4500ba7d50425e58ae34",
            "b8e433dc174248038d596faa8366193b",
            "3fe84832b8424a9ab5d567530711a0c4",
            "5b3b6a4e9dd54933abfecb8d86f85cd9",
            "e7c7f4d8575842ca8e7b06501d373d64"
          ]
        },
        "id": "e5Mnej3Lpu9n",
        "outputId": "914d4087-36b0-4134-f12a-3bd54390a228"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "529c43af0a7141278eb63b72c7e7cb89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "716d9e69e156456f8bfddce347fdcb6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17f55471fabc47ac91033a719fd305d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11c0185b048f45909d2686f3b3c8b1fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0990effb404648b883fb0ad3f272f251"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: RoBERTa Model\n",
        "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "steps = len(train_labels) // batch_size * epochs\n",
        "warmup = int(0.1 * steps)\n",
        "\n",
        "optimizer, _ = create_optimizer(init_lr=2e-5, num_train_steps=steps, num_warmup_steps=warmup)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "a22f89c0b5a64c308d22f27f7e72f167",
            "fcf274bb8b7744639d3af14b3ac19603",
            "3677846ac7264262a739565608aa2fbd",
            "c2dba2408739412bb7f9c755dcd9ce10",
            "aa4805c3158642949c3cb9aba61402a0",
            "8cd01fdf16274bc99f9d94e2597c4dd9",
            "2b256140ff6b4a4c9f80c71a9fb46f0a",
            "a0211d5cb4234f61ab1094ccd29e32c8",
            "54811c2453ee4f4b9a5663397c8ddb63",
            "d20224cf0c3f4e23866043117caedf3b",
            "1afa07bda6f04776865375dbc0a76f94"
          ]
        },
        "id": "7KL1MZCVpx-N",
        "outputId": "d50f1e88-b267-4f09-be63-d98b4ea8d5cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a22f89c0b5a64c308d22f27f7e72f167"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 8: Train RoBERTa\n",
        "model.fit(\n",
        "    [train_enc['input_ids'], train_enc['attention_mask']],\n",
        "    train_labels_tf,\n",
        "    validation_data=([test_enc['input_ids'], test_enc['attention_mask']], test_labels_tf),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptnXfKnYp2dm",
        "outputId": "e730062a-62c4-4758-b6a0-1c26b7e94b5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5654/5654 [==============================] - 1299s 218ms/step - loss: 0.4017 - accuracy: 0.8169 - val_loss: 0.3498 - val_accuracy: 0.8494\n",
            "Epoch 2/5\n",
            "5654/5654 [==============================] - 1256s 222ms/step - loss: 0.3363 - accuracy: 0.8572 - val_loss: 0.3490 - val_accuracy: 0.8554\n",
            "Epoch 3/5\n",
            "5654/5654 [==============================] - 1213s 215ms/step - loss: 0.2989 - accuracy: 0.8748 - val_loss: 0.3470 - val_accuracy: 0.8520\n",
            "Epoch 4/5\n",
            "5654/5654 [==============================] - 1255s 222ms/step - loss: 0.2584 - accuracy: 0.8937 - val_loss: 0.3740 - val_accuracy: 0.8516\n",
            "Epoch 5/5\n",
            "5654/5654 [==============================] - 1255s 222ms/step - loss: 0.2202 - accuracy: 0.9113 - val_loss: 0.4029 - val_accuracy: 0.8499\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7aa55d727550>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate RoBERTa\n",
        "preds_bert = model.predict([test_enc['input_ids'], test_enc['attention_mask']])\n",
        "preds_bert = tf.argmax(preds_bert.logits, axis=1).numpy()\n",
        "\n",
        "print(\"\\n=== RoBERTa Classification Report ===\")\n",
        "print(classification_report(test_labels, preds_bert))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYZ7McmNp46l",
        "outputId": "83e02608-e779-4353-ae64-64fad68fc87a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "707/707 [==============================] - 84s 113ms/step\n",
            "\n",
            "=== RoBERTa Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85     11060\n",
            "           1       0.87      0.83      0.85     11554\n",
            "\n",
            "    accuracy                           0.85     22614\n",
            "   macro avg       0.85      0.85      0.85     22614\n",
            "weighted avg       0.85      0.85      0.85     22614\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: TF-IDF + Logistic Regression\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
        "X_test_tfidf = vectorizer.transform(test_texts)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_tfidf, train_labels)\n",
        "preds_lr = log_reg.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\n=== TF-IDF + Logistic Regression Report ===\")\n",
        "print(classification_report(test_labels, preds_lr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqOlGHjAp7dk",
        "outputId": "91145ffc-6f87-400e-8037-661e4c2c16d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TF-IDF + Logistic Regression Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84     11060\n",
            "           1       0.86      0.81      0.84     11554\n",
            "\n",
            "    accuracy                           0.84     22614\n",
            "   macro avg       0.84      0.84      0.84     22614\n",
            "weighted avg       0.84      0.84      0.84     22614\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Comparison\n",
        "acc_bert = accuracy_score(test_labels, preds_bert)\n",
        "acc_lr = accuracy_score(test_labels, preds_lr)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Accuracy Comparison:\")\n",
        "print(f\"\ud83d\udd39 RoBERTa Accuracy: {acc_bert:.4f}\")\n",
        "print(f\"\ud83d\udd39 TF-IDF + Logistic Regression Accuracy: {acc_lr:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhG4XyNjuAjV",
        "outputId": "7eff9b9b-e8a8-4741-8cd4-a67eab6f94d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca Accuracy Comparison:\n",
            "\ud83d\udd39 RoBERTa Accuracy: 0.8499\n",
            "\ud83d\udd39 TF-IDF + Logistic Regression Accuracy: 0.8367\n"
          ]
        }
      ]
    }
  ]
}